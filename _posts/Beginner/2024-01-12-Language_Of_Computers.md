---
layout: post
title: "The Language Of Computers"
date: 2024-01-12 06:38:00 00
categories: software-engineering beginner
tags: coding software-enginner scratch guid binary ascii unicode
author: manishtiwari25
image:
  path: /assets/img/headers/beginner/language_of_computers.webp
---

## TL;DR

In today's digital age, we interact with computers constantly, using them for everything from browsing the internet to sending emails to playing games. But have you ever wondered how these machines understand the information we feed them? The answer lies in a complex system of codes and encodings that allow computers to interpret the digital world.

In the first article, we discussed Computational Thinking, if you missed it I would strongly recommend [reading that](/posts/Software_Engineer-Beginner/).

## Binary: The Language of Computers

At the heart of this system lies binary, a language composed of only two digits: 0 and 1. These two digits form the basis of everything computers understand, from the basic operation of a light switch to the complex algorithms that power websites and applications.

Binary code is represented by sequences of 0s and 1s, each representing a specific electrical state. These sequences are called bits, and they are the fundamental building blocks of digital information.

For more information, You can watch the video from CS50 and also try [this](https://cdn.cs50.net/2016/x/psets/0/pset0/bulbs.html)

<iframe width="560" height="315" src="/assets/videos/binary.mp4" title="CS50 - Lecture 0 - Binary" sandbox frameborder="0" allow="accelerometer; encrypted-media; gyroscope; picture-in-picture"  allowfullscreen></iframe>

{% include article-ads.html %}

## ASCII: A Universal Alphabet for Computers

To represent the vast array of text and symbols we use, computers use a standardized encoding system called ASCII. ASCII stands for American Standard Code for Information Interchange, and it defines a mapping of 128 characters to their corresponding binary code.

These characters include letters, numbers, punctuation marks, and control symbols. ASCII is a universal encoding system, meaning that it is understood by all computers, regardless of their operating system or hardware.

<iframe width="560" height="315" src="/assets/videos/ascii.mp4" title="CS50 - Lecture 0 - Ascii" sandbox frameborder="0" allow="accelerometer; encrypted-media; gyroscope; picture-in-picture"  allowfullscreen></iframe>

{% include article-ads.html %}

## Unicode: Embracing Global Language Diversity

While ASCII is great for representing English text, it falls short when dealing with languages that use non-Latin characters, such as Chinese, Japanese, and Arabic. To address this, Unicode was developed, providing a comprehensive encoding scheme for over 130,000 characters across a wide range of languages.

Unicode is a complex and evolving system, but it has become the standard for representing text in modern computing environments. It allows computers to handle a vast array of languages with ease, making communication more inclusive and accessible.

<iframe width="560" height="315" src="/assets/videos/unicode.mp4" title="CS50 - Lecture 0 - Unicode" sandbox frameborder="0" allow="accelerometer; encrypted-media; gyroscope; picture-in-picture"  allowfullscreen></iframe>

{% include article-ads.html %}

## Other Encodings for Specialized Data

Beyond ASCII and Unicode, there are many other encodings used for specialized data types, such as images, audio, and video. These encodings take into account the unique characteristics of each data type and use specific methods to represent them digitally.

For instance, JPEG is a widely used encoding for images, while MP3 and AAC are commonly used for audio compression. These encodings help to conserve storage space and improve data transmission efficiency.

{% include article-ads.html %}

## Conclusion

The codes and encodings discussed in this blog post form the backbone of our digital world. They allow computers to interpret the information we provide, enabling us to create, store, and share data effortlessly. As technology continues to evolve, the development of new encodings will play a crucial role in ensuring that our digital experiences remain seamless and inclusive

## References And Credits

1. https://www.youtube.com/watch?v=3LPJfIKxwWc
2. https://pll.harvard.edu/course/cs50-introduction-computer-science
